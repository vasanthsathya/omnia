# Copyright 2025 Dell Inc. or its subsidiaries. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
---

# Usage: main.yml
provision_validation_vars:
  - "{{ role_path }}/../mapping/vars/main.yml"
  - "{{ role_path }}/../mtms/vars/main.yml"
  - "{{ role_path }}/../switch_based/vars/main.yml"
provision_shared_library_path: /opt/omnia/shared_libraries/provision
ansible_invalid_tag_fail_msg: "Please run the playbook with the tag 'management_layer' only. Other tags are not allowed."
ansible_tag_fail_msg: "Please run the playbook without specifying any tags, as this feature is currently disabled and will be available in a future update."
federated_provision_unsupported: "The federated_provision feature is not currently supported but will be available soon."

# Usage: validate_provision_container.yml
xcatd_service: "xcatd.service"
postgresql_service_rhel: "postgresql.service"
postgresql_service_ubuntu: "postgresql"
prepare_oim_execution_req: |
  "Failed. If run the discovery/discovery.yml playbook, please ensure that you run the prepare_oim/prepare_oim.yml playbook before
  executing the discovery/discovery.yml playbook. If you're encountering this issue while running the discovery_provision.yml playbook,
  please cleanup provision tasks by running the 'ansible-playbook utils/oim_cleanup.yml --tags provision'.
  After verifying your input files and Omnia Infrastructure Manager admin NIC configuration, re-run the playbook discovery_provision.yml."
xcat_path: /opt/xcat/bin
oim_os_redhat: "redhat"
oim_os_rocky: "rocky"
oim_os_ubuntu: "ubuntu"
oim_os_fedora: "fedora"
postgresql_start_fail_msg: "Failed to start postgresql services"
xcat_start_fail_msg: "Failed to start xcatd services"
service_retries: 3

# Usage: include_provision_config.yml
spec_file:
  - "{{ hostvars['localhost']['input_project_dir'] }}/network_spec.yml"
  - "{{ hostvars['localhost']['input_project_dir'] }}/server_spec.yml"
provision_config_filename: "{{ hostvars['localhost']['input_project_dir'] }}/provision_config.yml"
provision_config_syntax_fail_msg: "Failed. Syntax errors present in provision_config.yml. Fix errors and re-run playbook again."
spec_syntax_fail_msg: "Failed. Syntax errors present in either network_spec.yml or server_spec.yml. Fix the errors and re-run."

# Usage: include_telemetry_config.yml
telemetry_config_filename: "{{ hostvars['localhost']['input_project_dir'] }}/telemetry_config.yml"
telemetry_config_syntax_fail_msg: "Failed. Syntax errors present in telemetry_config.yml. Fix errors and re-run playbook again."

# Usage: include_high_availability_config.yml
high_availability_config_path: "{{ hostvars['localhost']['input_project_dir'] }}/high_availability_config.yml"
high_availability_config_syntax_fail_msg: "Failed. Syntax errors present in high_availability_config.yml. Fix errors and re-run playbook again."
nfs_not_configured: "When enabling OIM HA or K8s Service Cluster an NFS server must be configured.
  Please run the oim_cleanup.yml in utils and re-run the omnia_startup.sh script using the NFS option."
# nfs_not_configured: "When enabling OIM HA or Service Node HA or Heirarchical cluster or K8s Service Cluster an NFS server must be configured.
#   Please run the OIM cleanup in utils and re-run the Omnia startup script using the NFS option."

# Usage: include_composable_roles_config.yml
omnia_metadata_file: "/opt/omnia/.data/oim_metadata.yml"
service_node_metadata_path: "/opt/omnia/.data/service_node_metadata.yml"
metadata_perm: "0644"

# Usage: validate_image_tars.yml
omnia_images_tar_missing_msg: |
  Following Images tarball does not exist - {{ missing_tars }}.
  To support either OIM HA or heirarchical provisioning, these images tar file is required.
  To ensure all omnia base image tar files are present execute `utils/save_container_images.yml` utility playbook.

# invalid_share_option_msg: |
#   [ERROR] Invalid value \"{{ omnia_share_option }}\" provided for omnia share path type.
#   For hierarchical-based discovery and provisioning, this must be set to \"NFS\".
#   Please rerun 'omnia_startup.sh' and enter correct omnia share path type as \"NFS\"
#   to continue with hierarchical-based discovery and provisioning.
service_node_not_found_msg: |
  "The service role is not defined in the roles_config.yml, but enable_service_ha is set to True.
  Kindly define the service_node role in roles_config and rerun the playbook."
oim_ha_node_not_found_msg: |
  "The OIM HA role is not defined in the roles_config.yml, but enable_oim_ha is set to True.
  Kindly define the oim_ha_node role in roles_config and rerun the playbook."
nfs_internal_warning_msg: |
  "WARNING: It is recommended to use External NFS for hierarchical cluster.
  If you must use Internal NFS, ensure OIM has enough storage space and sufficient hardware resources."
service_node_no_parent_msg: |
  Failed. The service_node role is defined in roles_config.yml, but none of the compute layer groups have a parent field specified.
  The issue arises because the playbook is attempting to provision the compute layer, but none of the groups have a valid parent
  field defined. To fix this, define the parent field in the compute layer groups within roles_config.yml for hierarchical provisioning,
  and re-run the playbook.
  If all nodes are to be provisioned from oim, then comment out the service_node role in roles_config.yml and re-run the playbook.
ha_nfs_error_msg: |
  ERROR: Internal NFS is not supported with HA OIM or Service HA.
  Please configure External NFS for HA capabilities.
service_cluster_nfs_error_msg: |
  ERROR: Internal NFS is not supported for K8s Service Cluster, external NFS server must be configured.
  Please run the oim_cleanup.yml in utils and re-run the omnia_startup.sh script using the NFS(external) option.
warning_wait_time_10s: 10
hierarchical_provision_warning_msg: |
  "WARNING: The service_node role is defined in roles_config.yml, but none of the groups have a parent field specified.
  This execution is for compute layer provisioning, and no groups have a valid parent field defined. If this is intentional,
  you may ignore this warning and proceed with the playbook execution.
  If this is unintentional, stop the execution, define the parent field in the compute layer groups within roles_config.yml
  for hierarchical provisioning, and re-run the playbook."
# Usage: include_local_repo_config.yml
local_repo_config_file: "{{ hostvars['localhost']['input_project_dir'] }}/local_repo_config.yml"
software_config_file: "{{ hostvars['localhost']['input_project_dir'] }}/software_config.json"
invalid_software_config_fail_msg: |
  Failed. Please provide valid software_config.json file with cluster_os_type, cluster_os_version,
  iso_file_path and repo_config values.
local_repo_config_syntax_fail_msg: "Failed. Syntax errors present in local_repo_config.yml. Fix errors and re-run playbook again."
software_config_syntax_fail_msg: "Failed. Syntax errors present in software_config.json. Fix errors and re-run playbook again."
iso_file_path_missing_msg: "Incorrect iso_file_path provided. Make sure ISO file is present in the provided iso_file_path."
iso_file_path_success_msg: "iso_file_path validated"
iso_file_path_fail_msg: "Failed. Invalid iso_file_path: {{ iso_file_path }} provided. Make sure iso_file_path variable in software_config.json contains value
 mentioned in the variables cluster_type: {{ provision_os }} and cluster_os_version: {{ provision_os_version }} mentioned in software_config.json"

# Usage: validate_local_repo.yml
metadata_file_path: "/opt/omnia/offline/.data/metadata.yml"
local_repo_fail_msg: "Failed! Please run local_repo.yml before running discovery_provision.yml/prepare_oim.yml"
softwares_warning_msg: "[WARNING] software_config.json does not have any softwares. Hence softwares will not be installed on the nodes post provisioning."
repo_store_path_fail_msg: "Failed. {{ repo_store_path }} didn't exist. Please run local_repo.yml before running discovery_provision.yml/prepare_oim.yml"
repo_config_metadata_fail_msg: "Failed: Cannot change repo_config in subsequent runs. Please use the repo_config:{{ md_repo_config }} in software_config.json"
warning_wait_time: 30

# Usage: assign_network_interface.yml
network_interface_fail_msg: "Failed. Please provide a valid network interface type."
warning_msg_bmc_network_details_incomplete: "Warning, BMC network details incomplete. BMC discovery and support will be skipped.
Note: oim_nic_name, netmask_bits, static_range and dynamic_range are mandatory parameters under bmc_network in network_spec.yml for bmc discover"

# Usage: validate_admin_nic.yml
success_msg_admin_nic_details: "Admin nic details validated"
fail_msg_admin_nic_details: "Failed. Invalid admin_nic details (oim_nic_name, netmask_bits, static_range or dynamic_range) in network_spec file."
fail_msg_admin_netmask_bits: "Failed. Invalid Admin netmask_bits provided in network_spec file."
success_subnet: "The subnet provided is validated"
fail_subnet: "Failed. Please provide proper subnet with netmask {{ omnia_admin_netmask }} in provision_config.yml"
success_lom: " Network interface type is LOM"
fail_lom: "Failed. In case of LOM, admin_nic_subnet and bmc_nic_subnet can't be same. Please provide proper input"
admin_params_success_msg: "Successfully validated admin network params."
admin_params_failure_msg: "Failed. Please provide proper input parameters for admin network in network_spec file."
admin_uncorrelated_ip_fail_msg: "Failed. admin_uncorrelated_node_start_ip is invalid or not within admin network static range."
admin_correlation_fail_msg: "Failed. Invalid details provided, correlation_to_admin should true or false."
validate_ip_within_range: "{{ provision_shared_library_path }}/validation/validate_ip_within_range.py"
validate_nic_status: "Failed, please check the network interface status should be UP"
fail_msg_admin_static_netmask_bits: "Failed, Admin static range is not within the admin netmask range."
admin_nic_netmask_fail_msg: "Failed, Admin nic netmask should be same as netmask in network_spec file."
admin_range_ip_check_fail_msg: "Failed. : Admin network - static/dynamic ranges should be valid IP address (Eg. 192.168.1.1-198.168.1.254)."
fail_admin_ip_range: "Failed, Admin static and dynamic ranges should not overldap."
admin_nic_fail_msg: "NIC '{{ admin_nic }}' does not exist on the system. Provide valid admin_network details in network_spec.yml and re-run the playbook."
admin_nic_success_msg: "NIC '{{ admin_nic }}' exists on the system."
admin_nic_ip_fail_msg: "IP '{{ admin_nic_ip }}' is not assigned to NIC '{{ admin_nic }}'. Please configure the admin IP in OIM and re-run the playbook."
virtual_ip_fail_msg: "OIM HA is enabled; however, the virtual IP '{{ admin_nic_ip }}' is not assigned to NIC '{{ admin_nic }}'.
  Please update the high_availability_config.yml and re-run prepare_oim to set the virtual IP."
admin_nic_ip_success_msg: "IP '{{ admin_nic_ip }}' is assigned to NIC '{{ admin_nic }}'."

# Usage: validate_bmc_nic.yml
success_msg_bmc_nic_details: "BMC nic details validated"
bmc_params_success_msg: "Successfully validated bmc network params"
bmc_params_failure_msg: "Failed. Please provide proper input parameters for bmc network in network_spec file."
fail_msg_bmc_netmask_bits: "Failed. Invalid BMC netmask_bits provided in network_spec file."
fail_msg_bmc_nic_details: "Failed. Invalid bmc_network details (oim_nic_name, netmask_bits, static_range or dynamic_range) in network_spec file."
bmc_nic_start: "{{ bmc_nic_subnet.split('.')[0] + '.' + bmc_nic_subnet.split('.')[1] + '.'
+ pxe_nic_start_range.split('.')[-2] + '.' + pxe_nic_start_range.split('.')[-1] }}"
bmc_nic_end: "{{ bmc_nic_subnet.split('.')[0] + '.' + bmc_nic_subnet.split('.')[1] + '.'
+ pxe_nic_end_range.split('.')[-2] + '.' + pxe_nic_end_range.split('.')[-1] }}"
network_address_script: "{{ provision_shared_library_path }}/validation/validate_network_address.py"
reassignment_to_static_failure_msg: "Failed. Invalid details provided, reassignment_to_static should true or false."
fail_msg_bmc_static_netmask_bits: "Failed, BMC static range is not within the BMC netmask range."
bmc_range_ip_check_fail_msg: "Failed. : BMC network - static/dynamic ranges should be valid IP address (Eg. 192.168.1.1-198.168.1.254)."
bmc_static_ranges_overlap_failure_msg: |
  "Range is invalid. Start range should not be equal to end range for bmc_conversion_static_start_range, bmc_conversion_static_end_range
  in network_spec.yml"
bmc_nic_subnet_sucess_msg: "Successfully validated bmc_nic_subnet"
bmc_nic_subnet_fail_msg: "Failed. Please provide bmc_nic_subnet value ipv4 format in network_spec.yml file"

# Usage: validate_network_spec.yml
static_range_check_fail_msg: "Failed. static_range_check variable in network_spec should be withing the netmask provided."
cidr_fail_msg: "Failed. CIDR or netmask_bits are invalid. Please provide valid CIDR or netmask_bits (Eg. '192.168.1.0/24')."
network_gateway_fail_msg: "Failed. network_gateway in network_spec should be in proper format."
vlan_fail_msg: "Failed. vlan in network_spec should be in proper format."
netmask_bits_failure_msg: "Failed. admin and bmc netmask should be same."
netmask_bits_success_msg: "Validated admin and bmc netmask bits"
cidr_or_static_range_fail_msg: "Failed. network_spec should have either static_range or CIDR for the network."
fail_msg_netmask_bits: "Failed. Invalid netmask_bits provided in network_spec file."
ip_range_netmask_script_script: "{{ provision_shared_library_path }}/validation/validate_ip_range_netmask.py"
mtu_check_fail_msg: "Failed. MTU input variable in network_spec should be in proper integer format."
validate_cidr: "{{ provision_shared_library_path }}/validation/validate_cidr.py"
range_ip_check_fail_msg: "Failed. input ip range should be valid IP address (Eg. 192.168.1.1-198.168.1.254)."
fail_static_ip_range: "Failed, Network static overlaps with"
fail_cidr_ip_range: "Failed, Cidr overlaps with"

# Usage: validate_discovery_params.yml
validation_range_file: "{{ provision_shared_library_path }}/validation/validate_input_ranges.py"
management_layer_discovery_mechanism_fail_msg: |
  "Failed. Please provide valid details either for mapping or bmc discovery mechanism for the first layer of nodes and re-run the playbook.
  For mapping discovery, please provide the pxe_mapping_file_path in the provision_config.yml file.
  For bmc discovery, please provide the bmc_details in the roles_config.yml file."
default_layer_discovery_mechanism_fail_msg: |
  "Failed. Please provide valid details either for mapping, bmc or switch_based discovery mechanism for the nodes and re-run the playbook.
  For mapping discovery, please provide the pxe_mapping_file_path in the provision_config.yml file.
  For bmc static discovery, please provide the bmc_details in the roles_config.yml file.
  For bmc dynamic discovery, please provide the bmc_network details in the network_spec.yml file.
  For switch_based discovery, provide bmc_details and switch_details in roles_config.yml"
invalid_groups_msg: |
  "Failed, all discovery mechanism invalid for groups: [{{ failed_groups | list | join(', ') }}]
  For mapping discovery, please provide the pxe_mapping_file_path in the provision_config.yml file.
  For bmc static discovery, please provide the bmc_details in the roles_config.yml file.
  For bmc dynamic discovery, please provide the bmc_network details in the network_spec.yml file.
  For switch_based discovery, provide bmc_details and switch_details in roles_config.yml"

# Usage: validate_provision_vars.yml
input_provision_fail_msg: |
  "Failed! Please provide all the required provision parameters in provision_config.yml, namely,
  domain_name,timezone and default_lease_time in provision_config.yml."
default_lease_time_success_msg: "default_lease_time validated"
default_lease_time_fail_msg: "Failed. Please provide a valid default_lease_time"
timezone_file_path: "{{ provision_shared_library_path }}/validation/timezone.txt"
timezone_success_msg: "timezone validated"
timezone_fail_msg: "Failed. Incorrect timezone provided. Please check the file timezone.txt in files folder"
language_fail_msg: "Failed. Only en-US language supported"
os_supported_rocky: rocky
os_supported_rhel: rhel
os_supported_ubuntu: ubuntu
provision_os_success_msg: "cluster_os_type validated"
provision_os_fail_msg: |
  "Failed. Incorrect cluster_os_type selected.
  If Omnia Infrastructure Manager OS RHEL, only cluster_os_type {{ os_supported_rhel }} is supported."
supported_rhel_os_version: ["9.6"]
supported_ubuntu_os_version: ["20.04", "22.04", "24.04"]
provision_os_version_fail_msg: |
  "Failed. Invalid cluster_os_version: {{ provision_os_version }} provided in software_config.json.
  Supported cluster_os_version values for cluster_os_type rhel is only 9.6"
ubuntu_kernel_fail_msg: "Failed. ubuntu_kernel_flavor should be either hwe or generic"
ubuntu22_version: "22.04"
ubuntu24_version: "24.04"
ntp_support_fail_msg: "Failed. ntp_support in provision_config.yml should be either true or false"
disk_partition_success_msg: "disk_partition successfully validated"
disk_partition_fail_msg: "Failed. Duplicate disk_partition values present in provision_config.yml."

# Usage: validate_disk_partition_vars.yml
mount_point_success_msg: "mountpoint of disk_partition successfully validated"
mount_point_fail_msg: "Failed. Supported disk_partition mount_point values are /var, /tmp, /usr, swap"
desired_capacity_success_msg: "desired_capacity of disk_partition successfully validated"
desired_capacity_fail_msg: "Failed. Provide valid integer value to desired_capacity of disk_partition"

# Usage: validate_domain_name.yml
server_hostname_success_msg: "Hostname in server hostname validated"
server_hostname_fail_msg: "Failed. Hostname set is not valid"
hosts_file_path: "/opt/omnia/hosts"
hosts_file_mode: "0644"
hostname_success_msg: "Hostname length successfully validated."
hostname_fail_msg: "Failed. Hostname must not exceed 65 characters. The domain name and node name,
dynamically generated based on the group used by omnia, contribute to the total length.
The maximum length for the node name is 13 characters. Please set the domain name carefully to avoid any issues."

# Usage: validate_ofed_cuda_repo.yml
ofed_version_warning_msg: "[WARNING] software_config.json does not have the version for OFED.
Hence OFED will not be installed on the nodes post provisioning."
ofed_repo_warning_msg: "[WARNING] local_repo.yml is not executed for downloading OFED packages.
OFED will not be installed on the nodes post provisioning."
cuda_version_warning_msg: "[WARNING] software_config.json does not have the version for CUDA.
Hence CUDA will not be installed on the nodes post provisioning."
cuda_repo_warning_msg: "[WARNING] local_repo.yml is not executed for downloading CUDA packages.
CUDA will not be installed on the nodes post provisioning."
offline_iso_directory: "/opt/omnia/offline_repo/cluster/{{ provision_os }}/{{ provision_os_version }}/iso"

# Usage: validate_amdgpu_rocm_repo.yml
pulp_bin_path: /usr/local/bin/pulp
amdgpu_input_warning_msg: "[WARNING] software_config.json does not have the amdgpu software stack.
Hence ROCm will not be installed on the nodes post provisioning."
amdgpu_version_warning_msg: "[WARNING] software_config.json does not have the version for AMDGPU.
Hence ROCm will not be installed on the nodes post provisioning."
amdgpu_repo_warning_msg: "[WARNING] local_repo.yml is not executed for downloading AMDGPU packages.
ROCm will not be installed on the nodes post provisioning."
rocm_version_warning_msg: "[WARNING] software_config.json does not have the version for ROCM.
Hence ROCm will not be installed on the nodes post provisioning."
rocm_repo_warning_msg: "[WARNING] local_repo.yml is not executed for downloading ROCM packages.
ROCm will not be installed on the nodes post provisioning."
os_package_map:
  rhel: rpm
  rocky: rpm
  ubuntu: deb

# Usage: validate_service_nodes.yml
service_node_repo_failed_msg: "[Failed] It seems you have not executed local_repo.yml with service_node in software_config.yml.
service_node package is required for service_node role provisioning."
pulp_repo_fail_msg: "Failed. It seems pulp is not configured properly.
Run local_repo/local_repo.yml with required packages in software_config.yml."

# Usage: validate_racadm_repo.yml
racadm_warning_msg: |
  "[WARNING] local_repo.yml is not executed for downloading racadm packages.
  Hence, racadm will not be installed on the nodes post provisioning."
racadm_pulp_name: tarballracadm

# Usage: validate_intelgaudi_repo.yml
intelgaudi_version_warning_msg: "[WARNING] software_config.json does not have the version for 'intelgaudi'.
Hence Habana stack will not be installed on the nodes post provisioning."
intelgaudi_repo_warning_msg: "[WARNING] local_repo.yml is not executed for downloading 'intelgaudi' packages.
Habana stack will not be installed on the nodes post provisioning."

# Usage: validate_broadcom_repo.yml
roce_version_warning_msg: "[WARNING] software_config.json does not have the version for bcm_roce.
Hence RoCE drivers will not be installed on the nodes post provisioning."
roce_repo_warning_msg: "[WARNING] bcm_roce is mentioned in software_config.json and local_repo.yml not executed for current bcm_roce version.
RoCE drivers will not be installed on the nodes post provisioning."
roce_src_version_warning_msg: "[WARNING] software_config.json does not have the version for bcm_roce_libraries.
Hence RoCE libraries will not be installed on the nodes post provisioning."
roce_src_repo_warning_msg: "[WARNING] bcm_roce_libraries is mentioned in software_config.json and local_repo.yml not executed for current bcm_roce_libraries
 version. RoCE libraries will not be installed on the nodes post provisioning."

# Usage: validate_site_config.yml
site_config_file: "{{ hostvars['localhost']['input_project_dir'] }}/site_config.yml"
invalid_proxy_failure_msg: "Failed. Both http_proxy and https_proxy should be set for proxy variable provided in site_config.yml"
proxy_env_fail_msg: "Failed. The values for http_proxy {{ proxy[0].http_proxy }} and https_proxy {{ proxy[0].https_proxy }} in the
proxy variable of the site_config.yml should be set as environment variables http_proxy and https_proxy in the Omnia Infrastructure Manager.
The no_proxy environment variable should include the Omnia Infrastructure Manager hostname and the admin network IP address."
update_repos_fail_msg: "Failed to update repos. Verify proxy configuration in Omnia Infrastructure Manager for acccessing internet."
repo_retries: 5
repo_delay: 10
