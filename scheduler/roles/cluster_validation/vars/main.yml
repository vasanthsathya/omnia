#  Copyright 2025 Dell Inc. or its subsidiaries. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
---

# Usage: main.yml
input_project_dir: "{{ hostvars['localhost']['input_project_dir'] }}"
omnia_log_path: /var/log/omnia
oim_path: "/root/.ansible/oim/"
ansible_cfg_src: "{{ playbook_dir }}/ansible.cfg"
ansible_cfg_dest:
  - { path: "{{ playbook_dir }}/telemetry/ansible.cfg", log_path: "/var/log/omnia/omnia_telemetry.log", regexp: "/var/log/omnia.log" }
  - { path: "{{ playbook_dir }}/platforms/ansible.cfg", log_path: "/var/log/omnia/omnia_platforms.log", regexp: "/var/log/omnia.log" }

# Usage: append_ip_to_inventory.yml
k8s_nfs_share: "/opt/omnia/kubespray/{{ hostvars['127.0.0.1']['cluster_var_name'] }}/{{ hostvars['127.0.0.1']['cluster_name'] }}"

# Usage: include_local_repo_config.yml
local_repo_config_file: "{{ input_project_dir }}/local_repo_config.yml"
local_repo_config_syntax_fail_msg: "Failed. Syntax errors present in local_repo_config.yml. Fix errors and re-run playbook again."

# Usage: fetch_software_config.yml
software_config_json_file: "{{ input_project_dir }}/software_config.json"
local_repo_access_dest_path: "/opt/omnia/provision/local_repo_access.yml"

k8s_version_fail_msg: "Failed, Ensure version of {{ software_stack }} is mentioned in software_config.json"
k8s_packages_file: "{{ input_project_dir }}/config/{{ software_config.cluster_os_type }}/{{ software_config.cluster_os_version }}/{{ software_stack }}.json"
success_msg_ucx_version: "Success. ucx version is mentioned."
fail_msg_ucx_version: "Failed. ucx version is not provided in software_config.json. Please include ucx version in input/software_config.json and rerun the playbook." # noqa: yaml[line-length]
success_msg_openmpi_version: "Success. openmpi version is mentioned."
fail_msg_openmpi_version: "Failed. openmpi version is not provided in software_config.json. Please include openmpi version in input/software_config.json and rerun the playbook." # noqa: yaml[line-length]
compute_os_ubuntu: "ubuntu"
intel_gaudi_input_fail_msg: "Warning, software_config.json does not have the intel software stack. Intel stack will not be configured."
intel_gaudi_repo_fail_msg: "Failed, local_repo.yml is not executed for downloading Intel Gaudi driver packages."
warning_time: 30
offline_intel_directory: "{{ repo_store_path }}/cluster/apt"
intelgaudi_version_warning_msg: "Failed, Intel Gaudi version not found."
success_msg_nvidia_gpu_operator_version: "Success. nvidia gpu operator version is mentioned." # noqa: yaml[line-length]
fail_msg_nvidia_gpu_operator_version: "Failed. nvidia gpu operator version is not provided in software_config.json. Please include nvidia gpu operator version in input/software_config.json and rerun the playbook." # noqa: yaml[line-length]
fail_msg_k8s_support_false: "Failed. k8s is not present in software_config.json, please add k8s in input/software_config.json and rerun the playbook."  # noqa: yaml[line-length]

# Usage: validate_network_spec.yml
network_spec: "{{ input_project_dir }}/network_spec.yml"
network_spec_syntax_fail_msg: "Failed. Syntax errors present in network_spec.yml. Fix errors and re-run playbook again."
admin_nic_ip_fail_msg: "IP '{{ admin_nic_ip }}' is not assigned to NIC '{{ admin_nic }}'. Please configure the admin IP in OIM and re-run the playbook."
admin_nic_ip_success_msg: "IP '{{ admin_nic_ip }}' is assigned to NIC '{{ admin_nic }}'."

# Usage: include_high_availability_config.yml
ha_config_file: "{{ input_project_dir }}/high_availability_config.yml"
ha_config_syntax_fail_msg: "Failed. Syntax errors present in high_availability_config.yml. Fix errors and re-run playbook again."
ha_vip_address_fail_msg: "When k8s HA is enabled , provide k8s_head_node_ha.virtual_ip_address in high_availability_config.yml"

# Usage: fetch_omnia_inputs.yml
config_filename: "omnia_config.yml"
min_length: 8
max_length: 30
omnia_config_syntax_fail_msg: "Failed. Syntax errors present in omnia_config.yml. Fix errors and re-run playbook again."
success_msg_k8s_cni: "Kubernetes CNI Validated"
fail_msg_k8s_cni: "Kubernetes CNI not correct."
supported_topology_manager_policy: ['none', 'best-effort', 'restricted', 'single-numa-node']
success_msg_k8s_toplogy_manager_policy: "topology_manager_policy validated"
fail_msg_k8s_toplogy_manager_policy: "topology_manager_policy can either be 'none' or 'best-effort' or 'restricted' or 'single-numa-node' in omnia_config.yml"
supported_topology_manager_scope: ['pod', 'container']
success_msg_k8s_toplogy_manager_scope: "topology_manager_scope validated"
fail_msg_k8s_toplogy_manager_scope: "topology_manager_scope can either be 'pod' or 'container' in omnia_config.yml"
success_msg_pod_external_ip_range: "pod_external_ip_range validated"
fail_msg_pod_external_ip_range: "pod_external_ip_range is not given in correct format in omnia_config.yml"
success_msg_k8s_service_addresses: "k8s_service_addresses validated"
fail_msg_k8s_service_addresses: "k8s_service_addresses value not given in correct format in omnia_config.yml"
success_msg_k8s_pod_network_cidr: "k8s_pod_network_cidr validated"
fail_msg_k8s_pod_network_cidr: "k8s_pod_network_cidr is not given in correct format"
file_perm: '0755'
ldap_required_success_msg: "ldap_required variable successfully validated"
ldap_required_fail_msg: "Failed. ldap_required should be either true or false"
freeipa_required_success_msg: "freeipa_required variable sccessfully validated"
freeipa_required_fail_msg: "Failed. freeipa_required should be either true or false"
ldap_login_failure_msg: "Failed. Both login_node_required and ldap_required cannot be true"
ldap_freeipa_failure_msg: "Failed. Both ldap_required and freeipa_required cannot be true"
enable_omnia_nfs_success_msg: "enable_omnia_nfs successfully validated"
enable_omnia_nfs_fail_msg: "Failed. enable_omnia_nfs should be either true or false"
input_config_failure_msg: "None of the parameters in omnia_config.yml should be empty."
slurm_installation_type_empty_failure_msg: "Slurm Installation type cannot be empty in omnia_config.yml"
slurm_installation_type_wrong_failure_msg: "Slurm Installation Type should be either nfs_share or configless in omnia_config.yml"
restart_services_success_msg: "restart_slurm_services successfully validated"
restart_services_failure_msg: "Failed. restart_slurm_services accepts true or false in omnia_config.yml"

# Usage: set_facts.yml
deployment_false_msg: >
  None of the entries under '{{ cluster_type }}_k8s_cluster' have 'deployment' set to 'true'.
  Please update exactly one cluster under '{{ cluster_type }}_k8s_cluster' in omnia_config.yml with 'deployment: true'.
compute_service_k8s_true_fail_msg: "Both scheduler.yml and service_k8s_cluster.yml cannot be run together. Run only any one of playbook."
cluster_type_unknown_msg: "Neither compute_k8s_playbook nor service_k8s_playbook is set."

# Usage: fetch_storage_config.yml
storage_config_filename: "storage_config.yml"
storage_config_syntax_fail_msg: "Failed. Syntax errors present in storage_config.yml. Fix errors and re-run playbook again."
nfs_client_params_failure_msg: "nfs_client_params variable can not be kept empty in input/storage_config.yml. It should have atleast one nfs share details."
nfs_client_params_k8s_share_fail_msg: "Exactly one entry should be present in nfs_client_params with k8s_share as true in input/storage_config.yml"
nfs_client_params_k8s_share_success_msg: "Entry found in nfs_client_params with k8s_share as true"
nfs_client_params_slurm_share_fail_msg: "Exactly one entry should be present in nfs_client_params with slurm_share as true in input/storage_config.yml"
nfs_client_params_slurm_share_success_msg: "Entry found in nfs_client_params with slurm_share as true"
nfs_client_params_benchmarks_fail_msg: "Atleast one out of k8s_share or slurm_share should be true in input/storage_config.yml when ucx/openmpi are installed on cluster nodes." # noqa: yaml[line-length]
nfs_client_params_benchmarks_success_msg: "Entry found in nfs_client_params with slurm_share or k8s_share as true"
plugins_warning_msg: |
  WARNING: Additional installation of Plugins will not be deployed because k8s nfs server_ip is "{{ k8s_nfs_server_ip }}".
  Please configure a valid NFS server IP in storage_config.yml.
nfs_client_params_server_share_invalid_msg: "No valid 'server_share_path' found with 'k8s_share: true'. Please provide a valid server_share_path in 'nfs_client_params' under 'storage_config.yml'" # noqa: yaml[line-length]

# Usage: validate_scheduler_type.yml
scheduler_type_success_msg: "scheduler_type successfully validated"
scheduler_type_fail_msg: "Failed. Invalid scheduler_type in omnia_config.yml. To install slurm provide scheduler_type: slurm
To install k8s provide scheduler_type: k8s. To install slurm and k8s provide scheduler_type: slurm,k8s"
install_scheduler_msg: "Installing job scheduler:"

# # Usage: Fetch_software_config.yml
# csi_driver_powerscale_packages_file: >-
#   {{ role_path }}/../../../input/config/{{ software_config.cluster_os_type }}/{{ software_config.cluster_os_version }}/csi_driver_powerscale.json

# Usage: fetch_omnia_inputs.yml
csi_driver_secret_file_path_success_msg: "Success. csi_driver_secret_file_path is valid in omnia_config.yml"
csi_driver_secret_file_path_fail_msg: "Failed. csi_driver_secret_file_path is not valid in omnia_config.yml. Please verify the path."

csi_driver_values_file_path_success_msg: "Success. csi_driver_values_file_path is valid in omnia_config.yml"
csi_driver_values_file_path_fail_msg: "Failed. csi_driver_values_file_path is not valid in omnia_config.yml. Please verify the path."

# Usage: csi_powerscale_driver_input_validation.yml
csi_powerscale_secret_vaultname: ".csi_powerscale_secret_vault"
fail_msg_isilon_clusters: "isilonClusters must be a valid list of powerscale details in secret.yaml file."
fail_msg_cluster_name: "clusterName is not valid. Provide powerscale cluster name in secret.yaml file."
fail_msg_user_name: "userName is not valid. Provide powerscale user name in secret.yaml file."
fail_msg_password: "Password is not valid. Provide powerscale password in secret.yaml file."
fail_msg_endpoint: "Endpoint is not valid. Provide powerscale IP or hostname in secret.yaml file."
fail_msg_endpoint_port: "endpointPort is not valid. Provide valid port number in secret.yaml file."
fail_msg_isdefault: "isDefault value should be true or false in secret.yaml file."
fail_msg_skip_certificate_validation: "skipCertificateValidation must be true in secret.yaml file."
fail_msg_isipath: "isiPath must be a valid Unix absolute path in secret.yaml file."
fail_msg_isi_volume_path_permissions: "isiVolumePathPermissions must be a valid directory permission (example: 0777) in secret.yaml file."
fail_msg_api_call: "Please recheck powerscale username, password, endpoint and endpointPort details provided in secret.yaml and
  values.yaml (if endpointPort is provided only in values.yaml) file. API call to powerscale was not successful"
vault_key_permission: "0644"


# Usage: validate_k8s_metadata.yml
metadata_file: "/opt/omnia/kubespray/k8s_metadata.yml"
k8s_metadata_file_not_exist_msg: "The metadata file {{ metadata_file }} does not exist. Please run prepare_oim.yml and rerun playbook."
k8s_version_failed_msg: >-
  {{ software_stack }} version {{ k8s_version }} not found in metadata file.
  Available versions are {{ metadata_dict[software_stack + '_kubespray_versions'].keys() | sort | join(', ') }}.
  Update the software_config.json with the available versions and rerun scheduler.yml,
  or run prepare_oim.yml to deploy the required kubespray container for {{ software_stack }}.

# Usage: validate_k8s_local_repo_metadata.yml
k8s_local_repo_metadata_file: "/opt/omnia/log/local_repo/k8s_local_repo_metadata.yml"
k8s_local_repo_metadata_file_not_exist_msg: "The metadata file {{ k8s_local_repo_metadata_file }} does not exist. Please run local_repo.yml and rerun playbook."
msg_not_in_list: "{{ software_stack }} version {{ k8s_version }} is not present in {{ k8s_local_repo_metadata_file }}! Please run local_repo.yml and rerun playbook" # noqa: yaml[line-length]
msg_version_mismatch: >-
  Last execution of local_repo.yml with {{ software_stack }} was
  {{ metadata_yaml['last_' + software_stack + '_local_repo_version'] }},
  but you requested version {{ k8s_version }} for {{ software_stack }} deployment.
  Please rerun local_repo.yml with version {{ k8s_version }}.
k8s_version_msg_success: "{{ software_stack }} version {{ k8s_version }} is valid and matches last_{{ software_stack }}_local_repo_version"
