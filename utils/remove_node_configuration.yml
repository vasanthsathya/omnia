#  Copyright 2025 Dell Inc. or its subsidiaries. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
---

- name: Include input project directory
  ansible.builtin.import_playbook: ../utils/include_input_dir.yml
  tags: always

- name: Warning and User confirmation for removing slurm and kube node
  hosts: localhost
  connection: local
  gather_facts: false
  vars:
    remove_node: true
  roles:
    - { role: remove_node/user_confirmation } # noqa: role-name[path]
    - { role: common }
  tags: always

- name: Create provision container group
  ansible.builtin.import_playbook: ../utils/create_container_group.yml
  vars:
    omnia_provision_group: true
    omnia_provision_validation: true
  tags: always

- name: Host Mapping
  hosts: omnia_provision
  connection: ssh
  roles:
    - servicetag_host_mapping
  tags: always

- name: Proceeding to remove slurm login node
  tags: login_node
  hosts: localhost
  connection: ssh
  gather_facts: true
  roles:
    - { role: remove_node/validate_slurm_node } # noqa: role-name[path]

- name: Remove login node
  tags: login_node
  hosts: remove_login_node
  connection: ssh
  gather_facts: true
  roles:
    - { role: remove_node/remove_slurm_node } # noqa: role-name[path]

- name: Proceeding to remove slurm node
  tags: slurm_node
  hosts: localhost
  connection: ssh
  gather_facts: false
  roles:
    - { role: remove_node/validate_slurm_node } # noqa: role-name[path]

- name: Remove slurm node
  tags: slurm_node
  hosts: remove_slurm_node
  connection: ssh
  gather_facts: true
  roles:
    - { role: remove_node/remove_slurm_node } # noqa: role-name[path]

- name: Create kubespray container group
  tags: kube_node
  ansible.builtin.import_playbook: ../utils/create_container_group.yml
  vars:
    omnia_kubespray_group: true
    omnia_kubespray_validation: true

- name: Proceeding to remove kube node
  tags: kube_node
  hosts: omnia_provision
  connection: ssh
  gather_facts: false
  roles:
    - { role: remove_node/remove_kube_node } # noqa: role-name[path]
  tasks:
    - name: Add host
      when: removing_nodes is defined and removing_nodes | length > 0
      ansible.builtin.add_host:
        name: "remove_nodes"
        nodes_removing: "{{ removing_nodes }}"

- name: Remove kube nodes
  hosts: omnia_kubespray
  tags: kube_node
  vars:
    node: "{{ hostvars['remove_nodes']['nodes_removing'] }}"
    user_inventory_path: "{{ ansible_inventory_sources[0] }}"
    kubespray_nfs_share: "/opt/omnia/kubespray/{{ hostvars['127.0.0.1']['cluster_var_name'] }}/{{ hostvars['127.0.0.1']['cluster_name'] }}"
    kubespray_log_path: "/opt/omnia/log/kubespray/{{ hostvars['127.0.0.1']['cluster_var_name'] }}/{{ hostvars['127.0.0.1']['cluster_name'] }}/remove_kube_node.yml" # noqa: yaml[line-length]
    dir_mode: '0644'
  tasks:
    - name: Execute tasks for Remove kube nodes
      when: hostvars['remove_nodes']['nodes_removing'] is defined
      block:
        - name: Copy remove_kube_node to kubespray nfs share
          ansible.builtin.copy:
            src: "{{ playbook_dir }}/playbooks/remove_kube_node.yml"
            dest: "{{ kubespray_nfs_share }}/remove_kube_node.yml"
            mode: "{{ dir_mode }}"

        - name: Inventory provided ,copy user inventory to nfs share
          ansible.builtin.copy:
            src: "{{ user_inventory_path }}"
            dest: "{{ kubespray_nfs_share }}/inv_remove_k8s"
            mode: "{{ dir_mode }}"
          when:
            - user_inventory_path is defined
            - (user_inventory_path | length > 0)

        - name: Execute ansible-playbook for Kubernetes remove node asynchronously
          ansible.builtin.shell:
            cmd: >
              set -o pipefail &&
              /venv/bin/ansible-playbook {{ kubespray_nfs_share }}/remove_kube_node.yml -i {{ kubespray_nfs_share }}/inv_remove_k8s
              --extra-vars "@{{ kubespray_nfs_share }}/k8s_all_vars.yml"
              --extra-vars "node={{ node }}" -vvv | /usr/bin/tee {{ kubespray_log_path }}
          async: 3600  # Set async timeout (e.g., 1 hour)
          poll: 0  # Non-blocking (continue the playbook without waiting for completion)
          register: k8s_remove_result  # Register the result to capture job ID
          changed_when: false

        - name: Wait for the Kubernetes remove kube_nodes to finish. Logs can be checked at {{ kubespray_log_path }}
          ansible.builtin.async_status:
            jid: "{{ k8s_remove_result.ansible_job_id }}"  # Job ID from the previous task
          register: job_result
          until: job_result.finished
          retries: 60  # Retry the task 60 times (1 hour total)
          delay: 10  # Wait 10 seconds between retries

# manually removing kube service address from /etc/resolv.conf
- name: Remove kube service address from kube nodes
  tags: kube_node
  hosts: remove_kube_node
  connection: ssh
  gather_facts: true
  roles:
    - { role: remove_node/post_kube_node_removal } # noqa: role-name[path]

- name: Confirm slurm_node removal
  hosts: remove_slurm_node
  connection: ssh
  gather_facts: false
  tags: slurm_node
  roles:
    - { role: remove_node/verify_slurm_node_removal } # noqa: role-name[path]

- name: Confirm kube_node removal
  hosts: remove_kube_node
  connection: ssh
  gather_facts: false
  tags: kube_node
  roles:
    - { role: remove_node/verify_kube_node_removal } # noqa: role-name[path]
