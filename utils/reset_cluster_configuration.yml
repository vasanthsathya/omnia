#  Copyright 2025 Dell Inc. or its subsidiaries. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
---

- name: Include input project directory
  ansible.builtin.import_playbook: ../utils/include_input_dir.yml
  tags: always

- name: Warning and User confirmation for removing cluster
  hosts: localhost
  connection: local
  gather_facts: false
  vars:
    reset_cluster: true
  roles:
    - { role: remove_cluster/user_confirmation } # noqa: role-name[path]
    - { role: common }
  tags: always

- name: Create provision container group
  ansible.builtin.import_playbook: ../utils/create_container_group.yml
  vars:
    omnia_provision_group: true
    omnia_provision_validation: true
  tags: always

- name: Host Mapping
  hosts: omnia_provision
  connection: ssh
  roles:
    - servicetag_host_mapping
  tags: always

- name: Proceeding to remove slurm cluster
  hosts: slurm_node, login, slurm_control_node
  connection: ssh
  gather_facts: true
  tags: slurm_cluster
  roles:
    - { role: remove_cluster/remove_slurm_cluster } # noqa: role-name[path]

- name: Create kubespray container group
  tags: k8s_cluster
  ansible.builtin.import_playbook: ../utils/create_container_group.yml
  vars:
    omnia_kubespray_group: true
    omnia_kubespray_validation: true
  when: hostvars['localhost']['k8s_support']

- name: Proceeding to remove kubernets cluster
  tags: k8s_cluster
  hosts: localhost
  connection: ssh
  gather_facts: false
  roles:
    - { role: remove_cluster/remove_k8s_cluster } # noqa: role-name[path]

- name: Reset kubernetes cluster
  tags: k8s_cluster
  hosts: omnia_kubespray
  vars:
    user_inventory_path: "{{ ansible_inventory_sources[0] }}"
    kubespray_nfs_share: "/opt/omnia/kubespray/{{ hostvars['127.0.0.1']['cluster_var_name'] }}/{{ hostvars['127.0.0.1']['cluster_name'] }}"
    kubespray_log_path: "/opt/omnia/log/kubespray/{{ hostvars['127.0.0.1']['cluster_var_name'] }}/{{ hostvars['127.0.0.1']['cluster_name'] }}/k8s_reset_cluster.log" # noqa: yaml[line-length]
    dir_mode: '0644'
  tasks:
    - name: Execute tasks for Reset kubernetes setup
      when: hostvars['localhost']['k8s_support']
      block:
        - name: Copy reset_k8s_cluste to kubespray nfs share
          ansible.builtin.copy:
            src: "{{ playbook_dir }}/playbooks/reset_k8s_cluster.yml"
            dest: "{{ kubespray_nfs_share }}/reset_k8s_cluster.yml"
            mode: "{{ dir_mode }}"

        - name: Inventory provided ,copy user inventory to nfs share
          ansible.builtin.copy:
            src: "{{ user_inventory_path }}"
            dest: "{{ kubespray_nfs_share }}/inv_reset_k8s"
            mode: "{{ dir_mode }}"
          when:
            - user_inventory_path is defined
            - (user_inventory_path | length > 0)

        - name: Execute ansible-playbook for Kubernetes reset asynchronously
          ansible.builtin.shell:
            cmd: >-
              set -o pipefail &&
              /venv/bin/ansible-playbook {{ kubespray_nfs_share }}/reset_k8s_cluster.yml
              -i {{ kubespray_nfs_share }}/inv_reset_k8s
              --extra-vars "@{{ kubespray_nfs_share }}/k8s_all_vars.yml" -vvv
              | /usr/bin/tee {{ kubespray_log_path }}
          async: 3600  # Set async timeout (e.g., 1 hour)
          poll: 0  # Non-blocking (continue the playbook without waiting for completion)
          register: k8s_reset_result  # Register the result to capture job ID
          changed_when: false

        - name: Wait for the Kubernetes reset cluster to finish. Logs can be checked at {{ kubespray_log_path }}
          ansible.builtin.async_status:
            jid: "{{ k8s_reset_result.ansible_job_id }}"  # Job ID from the previous task
          register: job_result
          until: job_result.finished
          retries: 60  # Retry the task 60 times (10 min total)
          delay: 10  # Wait 10 seconds between retries

# manually removing kube service address from /etc/resolv.conf
- name: Remove kube service address post kubernetes cluster removal
  tags: k8s_cluster
  hosts: kube_node, kube_control_plane, etcd
  connection: ssh
  gather_facts: true
  roles:
    - { role: remove_cluster/post_k8s_cluster_removal } # noqa: role-name[path]

- name: Confirm slurm cluster removal
  hosts: slurm_control_node
  connection: ssh
  gather_facts: false
  tags: slurm_cluster
  roles:
    - { role: remove_cluster/verify_slurm_cluster } # noqa: role-name[path]

- name: Confirm kubernetes cluster removal
  hosts: kube_control_plane
  connection: ssh
  gather_facts: false
  tags: k8s_cluster
  roles:
    - { role: remove_cluster/verify_k8s_cluster } # noqa: role-name[path]
