# Copyright 2025 Dell Inc. or its subsidiaries. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---

# Usage: deploy_pcs_container.yml
pcs_image_name: "omnia_pcs"
pcs_image_tag: "latest"
pcs_container_name: "omnia_pcs"
read_write_mode: "0644"
read_write_execute_mode: "0744"
pcs_container_dir: "{{ oim_shared_path }}/omnia/pcs"
pcs_container_corosync_log_dir: "{{ oim_shared_path }}/omnia/log/pcs/corosync"
pcs_container_pacemaker_log_dir: "{{ oim_shared_path }}/omnia/log/pcs/pacemaker"
container_systemd_dir: "/etc/containers/systemd"
network_spec: "{{ hostvars['localhost']['input_project_dir'] }}/network_spec.yml"
network_spec_syntax_fail_msg: "Failed. Syntax errors present in network_spec.yml. Fix errors and re-run playbook again."
pcs_directories:
  - { path: "{{ pcs_container_corosync_log_dir }}", mode: "{{ read_write_execute_mode }}" }
  - { path: "{{ pcs_container_pacemaker_log_dir }}", mode: "{{ read_write_execute_mode }}" }
  - { path: "{{ pcs_container_dir }}/corosync", mode: "{{ read_write_execute_mode }}" }
  - { path: "{{ container_systemd_dir }}", mode: "{{ read_write_mode }}" }

pcs_startup_file:
  - {template: "pcs-start.sh.j2", dest: "{{ oim_shared_path }}/omnia/pcs/pcs-start.sh", mode: "{{ read_write_execute_mode }}" }
  - {template: "corosync.conf.j2", dest: "{{ oim_shared_path }}/omnia/pcs/corosync/corosync.conf", mode: "{{ read_write_mode }}" }
  - {template: "pcs-container.container.j2", dest: "{{ oim_shared_path }}/omnia/pcs/{{ pcs_container_name }}.container", mode: "{{ read_write_mode }}" }
  - {template: "pcs-container.container.j2", dest: "{{ container_systemd_dir }}/{{ pcs_container_name }}.container", mode: "{{ read_write_mode }}" }

pcs_resource_script:
  - {template: "pcs-container-resource-init.sh.j2", dest: "{{ oim_shared_path }}/omnia/pcs/pcs-start.sh", mode: "{{ read_write_execute_mode }}" }

pcs_image_pull_fail_msg: >
  The pull of the provision image {{ pcs_image_name }}:{{ pcs_image_tag }} has failed.
  To resolve this issue, please address the following error message: {{ image_pull_result }}.
  After rectifying the issues, you can re-run the playbook to successfully pull the provision image.
pcs_image_not_found_msg: "The specified image {{ pcs_image_name }}:{{ pcs_image_tag }} is not found."

wait_time: 10
pcs_resource_script_path: "/opt/omnia/pcs/pcs-start.sh"
pcs_volumes:
  - /etc/localtime:/etc/localtime:ro
  - /run/podman/podman.sock:/run/podman/podman.sock:ro
  - "{{ pcs_container_dir }}/pcs-start.sh:{{ pcs_resource_script_path }}{{ selinux_option }}"
  - "{{ pcs_container_dir }}/corosync:/etc/corosync{{ selinux_option }}"
  - "{{ pcs_container_corosync_log_dir }}:/var/log/cluster{{ selinux_option }}"
  - "{{ pcs_container_pacemaker_log_dir }}:/var/log/pacemaker{{ selinux_option }}"
  - "{{ oim_shared_path }}/omnia:/opt/omnia{{ selinux_option }}"

pcs_container_success_msg: "The {{ pcs_container_name }} container has been successfully deployed."
pcs_container_failure_msg: |
  The deployment of the {{ pcs_container_name }} container has failed. To resolve this issue,
  please run the utility/oim_cleanup.yml playbook to clean up any existing OIM resources.
  After the cleanup, you can re-run the original playbook to deploy the {{ pcs_container_name }} container successfully.

# Usage: pcs-start.sh.j2:Common
omnia_nfs_share: "{{ oim_shared_path }}/omnia" # Define NFS share path
oim_metadata_file: "{{ omnia_nfs_share }}/.data/oim_metadata.yml"
monitor_interval: "30s"
monitor_timeout: "60s"
start_interval: "0s"
start_timeout: "20s"
stop_interval: "0s"
stop_timeout: "60s"

migration_threshold: 0
ha_migration_threshold: 1
failure_timeout: "60s"
pcs_group: omnia
vip_group: omnia_vip

# Usage: pcs-start.sh.j2: omnia_core
omnia_core_name: "omnia_core"
core_container_image_tag: "latest"
core_container_name: "omnia_core"
omnia_core_image: "{{ omnia_core_name }}:{{ core_container_image_tag }}"
core_container_logs_dir: "{{ omnia_nfs_share }}/log/core/container"
pulp_ha_dir: "{{ omnia_nfs_share }}/pulp/pulp_ha"
omnia_core_run_opts: >-
  -dt --hostname {{ omnia_core_name }} --restart=always --network=host
  -v {{ omnia_nfs_share }}:/opt/omnia{{ selinux_option }}
  -v {{ omnia_nfs_share }}/ssh_config/.ssh:/root/.ssh{{ selinux_option }}
  -v {{ core_container_logs_dir }}:/var/log{{ selinux_option }}
  -v {{ hosts_file }}:/etc/hosts{{ selinux_option }}
  -v {{ pulp_ha_dir }}:/root/.config/pulp{{ selinux_option }}
  -e ROOT_PASSWORD_HASH={{ hostvars['localhost']['omnia_core_hashed_passwd'] }}
  --name {{ omnia_core_name }} --cap-add=CAP_AUDIT_WRITE

# Usage: pcs-start.sh.j2: omnia_provision
provision_container_name: "omnia_provision"
provision_image_name: "omnia_provision"
provision_image_tag: "latest"
provision_image: "{{ provision_image_name }}:{{ provision_image_tag }}"
provision_dir: "{{ omnia_nfs_share }}/provision"
xcatdata_dir: "{{ provision_dir }}/xcatdata"
pgsql_data_dir: "{{ xcatdata_dir }}/pgsql/data"
install_dir: "{{ provision_dir }}/install"
tftpboot_dir: "{{ provision_dir }}/tftpboot"
hosts_file: "{{ omnia_nfs_share }}/hosts"
provision_logs_dir: "{{ omnia_nfs_share }}/log/provision"
provision_run_opts: >-
  --network=host --restart=always --hostname {{ provision_container_name }}.{{ hostvars['localhost']['domain_name'] }} --privileged
  -v {{ omnia_nfs_share }}/ssh_config/.ssh:/root/.ssh{{ selinux_option }}
  -v {{ omnia_nfs_share }}:/opt/omnia{{ selinux_option }}
  -v /sys/fs/cgroup:/sys/fs/cgroup:ro
  -v {{ xcatdata_dir }}:/xcatdata{{ selinux_option }}
  -v {{ install_dir }}:/install{{ selinux_option }}
  -v {{ tftpboot_dir }}:/tftpboot{{ selinux_option }}
  -v {{ provision_logs_dir }}/xcat:/var/log/xcat{{ selinux_option }}
  -v {{ pgsql_data_dir }}:/var/lib/pgsql/data{{ selinux_option }}
  -v {{ hosts_file }}:/etc/hosts{{ selinux_option }}

# Usage: pcs-start.sh.j2: kubespray
kubespray_image_name: "omnia_kubespray"
kubespray_version: "{{ hostvars['localhost']['kubespray_version'] }}"
kubespray_name: "omnia_kubespray_{{ kubespray_version }}"
kubespray_container_image: "{{ kubespray_image_name }}:{{ kubespray_version }}"
kubespray_run_opts: >-
  -v {{ omnia_nfs_share }}/ssh_config/.ssh:/root/.ssh{{ selinux_option }}
  -v {{ omnia_nfs_share }}:/opt/omnia{{ selinux_option }}
  -v {{ hosts_file }}:/etc/hosts{{ selinux_option }}
  --network=host --restart=always --cap-add AUDIT_WRITE

# Usage: pcs-start.sh.j2: pulp
pulp_container_name: "pulp"
pulp_image: "docker.io/pulp/pulp:3.80"
pulp_port_http: "{{ pulp_server_port }}:80"
pulp_port_https: "{{ pulp_server_port }}:{{ pulp_server_port }}"
pulp_share: "{{ omnia_nfs_share }}/pulp"
pulp_run_opts_http: >-
  --hostname {{ pulp_container_name }} --device /dev/fuse:/dev/fuse:rwm
  -e PULP_WORKERS=10 -e PULP_API_WORKERS=10 -e PULP_CONTENT_WORKERS=10
  -e PULP_GUNICORN_TIMEOUT=30 -e PULP_API_WORKERS_MAX_REQUESTS=1000
  -e PULP_API_WORKERS_MAX_REQUESTS_JITTER=50 --restart=always --publish {{ pulp_port_http }}
  -v {{ pulp_share }}/settings:/etc/pulp{{ selinux_option }}
  -v {{ pulp_share }}/settings/pulp_storage:/var/lib/pulp{{ selinux_option }}
  -v {{ pulp_share }}/settings/pgsql:/var/lib/pgsql{{ selinux_option }}
  -v {{ pulp_share }}/settings/containers:/var/lib/containers{{ selinux_option }}
  -v {{ omnia_nfs_share }}/log/pulp:/var/log/pulp{{ selinux_option }}

pulp_run_opts_https: >-
  --hostname {{ pulp_container_name }} --device /dev/fuse:/dev/fuse:rwm
  -e PULP_WORKERS=10 -e PULP_API_WORKERS=10 -e PULP_CONTENT_WORKERS=10
  -e PULP_GUNICORN_TIMEOUT=30 -e PULP_API_WORKERS_MAX_REQUESTS=1000
  -e PULP_API_WORKERS_MAX_REQUESTS_JITTER=50 --restart=always --publish {{ pulp_port_https }}
  -v {{ pulp_share }}/settings:/etc/pulp{{ selinux_option }}
  -v {{ pulp_share }}/settings/pulp_storage:/var/lib/pulp{{ selinux_option }}
  -v {{ pulp_share }}/settings/pgsql:/var/lib/pgsql{{ selinux_option }}
  -v {{ pulp_share }}/settings/containers:/var/lib/containers{{ selinux_option }}
  -v {{ omnia_nfs_share }}/log/pulp:/var/log/pulp{{ selinux_option }}
  -v {{ pulp_share }}/pulp_ha:/root/.config/pulp{{ selinux_option }}
  -v {{ pulp_share }}/nginx/nginx.conf:/etc/nginx/nginx.conf:ro{{ selinux_option | replace(':', ',') }}

# Usage: pcs-start.sh.j2: squid
squid_container_name: "squid"
squid_name: "squid"
squid_image: "docker.io/ubuntu/squid:6.6-24.04_beta"
squid_port: "{{ hostvars['localhost']['admin_nic_ip'] }}:{{ hostvars['oim']['squid_proxy_port'] }}:{{ hostvars['oim']['squid_proxy_port'] }}"
squid_share: "{{ omnia_nfs_share }}/squid"
squid_run_opts: >-
  --hostname {{ squid_container_name }} --restart=on-failure --publish {{ squid_port }}
  -v {{ squid_share }}/cache:/var/spool/squid{{ selinux_option }}
  -v {{ squid_share }}/squid.conf:/etc/squid/squid.conf{{ selinux_option }}
  -v {{ omnia_nfs_share }}/log/squid:/var/log/squid{{ selinux_option }}

pcs_resources:
  - "{{ pulp_container_name }}"
  - "{{ provision_container_name }}"
  - "{{ omnia_core_name }}"
pcs_start_container_finished: "/opt/omnia/pcs/pcs-start.finished"
pcs_start_shared_finished: "{{ pcs_container_dir }}/pcs-start.finished"
pcs_container_resources_finished: "/opt/omnia/pcs/pcs-resources.finished"
pcs_container_resources_shared_finished: "{{ pcs_container_dir }}/pcs-resources.finished"
wait_time_pcs_start: 200
pcs_start_failed_msg: "pcs-start.sh execution failed."
pcs_failed_resource_msg: |
  The deployment of pcs resources have failed. To resolve this issue,
  please run the utility/oim_cleanup.yml playbook to clean up any existing OIM resources.
  After the cleanup, you can re-run the original playbook to deploy the {{ pcs_container_name }} container
  and its resources.

# Usage: pcs-container-resource-init.sh: idrac_telemetry
telemetry_pcs_group: omnia_telemetry
# mysqldb
mysql_env_vars:
  - "MYSQL_DATABASE={{ mysql_database }}"
mysql_run_opts: >-
  --restart=no
  --network=host
  --secret {{ mysql_root_password }},type=env,target=MYSQL_ROOT_PASSWORD
  --secret {{ mysql_user }},type=env,target=MYSQL_USER
  --secret {{ mysql_user_password }},type=env,target=MYSQL_PASSWORD
  --env {{ mysql_env_vars | join(' ') }}
  {{ mysqldb_volumes | map('regex_replace', '^', '--volume ') | join(' ') }}
  --name {{ mysql_container_name }}
  --entrypoint {{ mysql_entry_script }}
  --detach

# activemq
activemq_run_opts: >-
  --network=host
  --volume activemq_log:/opt/activemq/data{{ selinux_option }}
  --detach
  --name {{ activemq_container_name }}

# idrac_telemetry_receiver
idrac_telemetry_receiver_run_opts: >-
  --network=host
  --log-driver=json-file
  --log-opt path={{ idrac_telemetry_receiver_log_dir }}/idrac_telemetry_receiver.log
  --secret {{ mysql_user }},type=env,target=MYSQL_USER
  --secret {{ mysql_user_password }},type=env,target=MYSQL_PASSWORD
  {% for volume in idrac_telemetry_receiver_volumes %}
  --volume {{ volume }}
  {% endfor %}
  --workdir=/go/src/github.com/telemetry-reference-tools
  {% for key, value in idrac_telemetry_receiver_env.items() %}
  --env {{ key }}={{ value }}
  {% endfor %}
  --name {{ idrac_telemetry_receiver_container_name }}
  --entrypoint {{ idrac_telemetry_receiver_entry_script }}
  --detach

telemetry_group_resources:
  - "{{ mysql_container_name }}"
  - "{{ activemq_container_name }}"
  - "{{ idrac_telemetry_receiver_container_name }}"

# prometheus_pump
prometheus_pump_run_opts: >-
  --network=host
  {% for key, value in prometheus_pump_receiver_env.items() %}
  --env {{ key }}={{ value }}
  {% endfor %}
  {% for volume in prometheus_pump_volumes %}
  --volume {{ volume }}
  {% endfor %}
  --name {{ prometheus_pump_container_name }}
  -w /go/src/github.com/telemetry-reference-tools
  --entrypoint {{ prometheus_pump_entry_script }}
  --detach

# prometheus
prometheus_run_opts: >-
  --network=host
  --log-driver=json-file
  --log-opt path={{ prometheus_log_dir }}/prometheus.log
  {% for key, value in prometheus_env.items() %}
  --env {{ key }}={{ value }}
  {% endfor %}
  {% for volume in prometheus_volumes %}
  --volume {{ volume }}
  {% endfor %}
  --name {{ prometheus_container_name }}
  --detach

prometheus_group_resources:
  - "{{ prometheus_container_name }}"
  - "{{ prometheus_pump_container_name }}"

# Usage: pcs-container-resource-init.sh: visualization
visualization_pcs_group: omnia_visualization
# grafana
grafana_run_opts: >-
  --network=host
  --secret {{ grafana_user }},type=env,target=GF_SECURITY_ADMIN_USER
  --secret {{ grafana_user_password }},type=env,target=GF_SECURITY_ADMIN_PASSWORD
  --env GF_SERVER_HTTP_PORT={{ grafana_port }}
  --env GF_USERS_ALLOW_SIGN_UP=false
  --env GF_USERS_ALLOW_ORG_CREATE=false
  --env GF_LOG_MODE=file
  --env GF_PATHS_LOGS=/var/log/grafana
  {{ grafana_volumes | map('regex_replace', '^', '--volume ') | join(' ') }}
  --detach
  --entrypoint {{ grafana_entrypoint }}

# loki
container_config_mnt: "/mnt/config"
loki_run_opts: >-
  --network=host
  --restart=no
  --volume loki_promtail_config:{{ container_config_mnt }}{{ selinux_option }}
  --add-host=loki:127.0.0.1
  --add-host=promtail:127.0.0.1
  --detach
loki_run_cmd: >-
  --config.file={{ container_config_mnt }}/loki-config.yaml
loki_monitor_cmd: "/usr/bin/loki -version"

# promtail
promtail_run_opts: >-
  --network=host
  --restart=no
  --add-host=loki:127.0.0.1
  --add-host=promtail:127.0.0.1
  --volume loki_promtail_config:{{ container_config_mnt }}{{ selinux_option }}
  --volume {{ omnia_nfs_share }}:/opt/omnia{{ selinux_option }}
  --detach
promtail_run_cmd: >-
  --config.file={{ container_config_mnt }}/promtail-config.yaml
promtail_monitor_cmd: "/usr/bin/promtail -version"

visualization_group_resources:
  - "{{ grafana_container_name }}"
  - "{{ loki_container_name }}"
  - "{{ promtail_container_name }}"

secret_dir_permission: "0700"
podman_secrets_dir: "{{ oim_shared_path }}/omnia/.secrets"
podman_secrets_file:
  - { src: "/var/lib/containers/storage/secrets/secrets.json", dest: "{{ podman_secrets_dir }}/secrets.json", mode: "0600" }
  - { src: "/var/lib/containers/storage/secrets/filedriver/secretsdata.json", dest: "{{ podman_secrets_dir }}/secretsdata.json", mode: "0600" }
